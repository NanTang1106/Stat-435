col=c('skyblue', 'orange', 'orchid'), lty=1, lwd=3)
plot(x, y, col='gray', pch=16) +
lines(x, yhat1, lwd=3, col='skyblue') +
lines(x, yhat2, lwd=3, col='orange') +
lines(x, yhat3, lwd=3, col='orchid')
cv_fit <- cv.glmnet(X, y, alpha=0, lambda = lambda_range, penalty.factor = c(0, rep(1, train_size - 2), 0))
plot(cv_fit)
lambda_range <- c(0, 10^{seq(-3, 6, by=0.1)})
cv_fit <- cv.glmnet(X, y, alpha=0, lambda = lambda_range, penalty.factor = c(0, rep(1, train_size - 2), 0))
plot(cv_fit)
lambda_opt <- cv_fit$lambda.min
print(lambda_opt)
yhat <- X %*% solve( t(X) %*% X + lambda_opt * diag(c(0, rep(1, train_size - 2), 0), train_size, train_size)) %*% t(X) %*% y
rdg_fit_opt <- glmnet(X, y, alpha=0, lambda=lambda_opt, intercept=FALSE)
yhat <- predict(rdg_fit_opt, X)
plot(x, y, col='gray', pch=16)
lines(x, yhat, lwd=3, col='skyblue')
rdg_fit_opt <- glmnet(X, y, alpha=0, lambda=lambda_opt, intercept=FALSE, penalty.factor = c(0, rep(1, train_size - 2), 0))
yhat <- predict(rdg_fit_opt, X)
plot(x, y, col='gray', pch=16)
lines(x, yhat, lwd=3, col='skyblue')
plot(x, y, col='gray', pch=16)
lines(x, yhat, lwd=3, col='skyblue')
summary(cv_fit)
abline(lm(y~x-1), col='orange', lty=2, lwd=2)
plot(x, y, col='gray', pch=16)
lines(x, yhat4, lwd=3, col='skyblue')
abline(lm(y~x-1), col='orange', lty=2, lwd=2)
plot(x, y, col='gray', pch=16)
lines(x, yhat4, lwd=3, col='skyblue')
abline(lm(y~0 + x), col='orange', lty=2, lwd=2)
plot(x, y, col='gray', pch=16)
lines(x, yhat4, lwd=3, col='skyblue')
abline(lm(y~x), col='orange', lty=2, lwd=2)
legend('topright', legend=c('lambda = 10^6', 'linear OLS'),
col=c('skyblue', 'orange'), lty = c(1, 3))
plot(x, y, col='gray', pch=16)
lines(x, yhat4, lwd=3, col='skyblue')
abline(lm(y~x), col='orange', lty=2, lwd=2)
legend('topright', legend=c('lambda = 10^6', 'linear OLS'),
col=c('skyblue', 'orange'), lty = c(1, 3))
## verify lambda=10^6
yhat4 <- predict(rdg_fit, X, s=10^6)
plot(x, y, col='gray', pch=16)
lines(x, yhat4, lwd=3, col='skyblue')
abline(lm(y~x), col='orange', lty=2, lwd=2)
legend('topright', legend=c('lambda = 10^6', 'linear OLS'),
col=c('skyblue', 'orange'), lty = c(1, 3))
plot(x, y, col='gray', pch=16)
lines(x, yhat4, lwd=3, col='skyblue')
abline(lm(y~0+x), col='orange', lty=2, lwd=2)
legend('topright', legend=c('lambda = 10^6', 'linear OLS'),
col=c('skyblue', 'orange'), lty = c(1, 3))
## verify lambda=10^6
yhat4 <- predict(rdg_fit, X, s=10^6)
plot(x, y, col='gray', pch=16)
lines(x, yhat4, lwd=3, col='skyblue')
abline(lm(y~0+x), col='orange', lty=2, lwd=2)
legend('topright', legend=c('lambda = 10^6', 'linear OLS'),
col=c('skyblue', 'orange'), lty = c(1, 3))
plot(x, y, col='gray', pch=16)
lines(x, yhat4, lwd=3, col='skyblue')
abline(lm(y~x-1), col='orange', lty=2, lwd=2)
legend('topright', legend=c('lambda = 10^6', 'linear OLS'),
col=c('skyblue', 'orange'), lty = c(1, 3))
plot(x, y, col='gray', pch=16)
lines(x, yhat4, lwd=3, col='skyblue')
abline(lm(y~0 + x), col='orange', lty=2, lwd=2)
legend('topright', legend=c('lambda = 10^6', 'linear OLS'),
col=c('skyblue', 'orange'), lty = c(1, 3))
plot(x, y, col='gray', pch=16)
plot(x, y, col='gray', pch=16)
lines(x, yhat, lwd=3, col='skyblue')
library(leaps)
library(scales)
library(glmnet)
source('test-data.r')
truncated.power.design.matrix <- function(x) {
n <- length(x)
results <- matrix(0, n, n)
results[, n] <- 1
knots <- sort(x)
for (i in 1:n) {
x_i = x[i]
for (j in 1:(n-1)) {
knot_j <- knots[j]
if (x_i > knot_j) {
results[i, j] <- x_i - knot_j
} else {
break
}
}
}
return(results)
}
new_gcv <- numeric(train_size)
train_size=100
gcv <- numeric(train_size)
for (i in 1:train_size) {
dk = i + 1
gcv_temp <- knot_rss[i] / (train_size * (1 - dk/train_size)^2)
gcv[i] <- gcv_temp
}
train_size <- length(x)
knot_rss <- numeric(train_size)
X <- truncated.power.design.matrix(x)
for (i in 1:train_size) {
yhat <- regsubset.fitted.values(X, y, i)
rss_temp <- sum((y - yhat)^2)
knot_rss[i] <- rss_temp
}
regsubset.fitted.values <- function(X, y, nterm) {
reg_out <- regsubsets(X, y, nvmax=nterm, method='forward', intercept=FALSE)
knot_dm <- X[, which(summary(reg_out)$which[nterm,])]
yhat <- knot_dm %*% solve(t(knot_dm) %*% knot_dm) %*% t(knot_dm) %*% y
return(yhat)
}
train_size <- length(x)
knot_rss <- numeric(train_size)
X <- truncated.power.design.matrix(x)
for (i in 1:train_size) {
yhat <- regsubset.fitted.values(X, y, i)
rss_temp <- sum((y - yhat)^2)
knot_rss[i] <- rss_temp
}
plot(knot_rss, type='p', pch=16, cex=0.6,
xlab='Number of Knots', ylab='Training RSS')
gcv <- numeric(train_size)
for (i in 1:train_size) {
dk = i + 1
gcv_temp <- knot_rss[i] / (train_size * (1 - dk/train_size)^2)
gcv[i] <- gcv_temp
}
plot(gcv, type='p', pch=16, cex=0.6,
xlab='Number of Knots', ylab='GCV score')
new_gcv <- numeric(train_size)
for (i in 1:train_size) {
dk = 3*i + 1
gcv_temp <- knot_rss[i] / (train_size * (1 - dk/train_size)^2)
new_gcv[i] <- gcv_temp
}
plot(new_gcv, type='p', pch=16, cex=0.6,
xlab='Number of Knots', ylab='GCV score')
## for k = 30 at most
plot(new_gcv[1:30], type='p', pch=16, cex=0.6,
xlab='Number of Knots', ylab='GCV score')
train_size <- length(x)
X <- truncated.power.design.matrix(x)
lambda_range <- c(0, 1, 10, 10^6)
rdg_fit <- glmnet(X, y, alpha=0, lambda=lambda_range, intercept=FALSE,
penalty.factor = c(0, rep(1, train_size - 2), 0))
yhat1 <- predict(rdg_fit, X, s=0)
yhat2 <- predict(rdg_fit, X, s=1)
yhat3 <- predict(rdg_fit, X, s=10)
plot(x, y, col='gray', pch=16)
lines(x, yhat1, lwd=3, col='skyblue')
lines(x, yhat2, lwd=3, col='orange')
lines(x, yhat3, lwd=3, col='orchid')
legend('topright', legend = c('lambda = 0', 'lambda = 1', 'lambda = 10'),
col=c('skyblue', 'orange', 'orchid'), lty=1, lwd=3)
## verify lambda=10^6
## use OLS without intercept
yhat4 <- predict(rdg_fit, X, s=10^6)
plot(x, y, col='gray', pch=16)
lines(x, yhat4, lwd=3, col='skyblue')
abline(lm(y~0 + x), col='orange', lty=2, lwd=2)
legend('topright', legend=c('lambda = 10^6', 'linear OLS'),
col=c('skyblue', 'orange'), lty = c(1, 3))
train_size <- length(x)
X <- truncated.power.design.matrix(x)
lambda_range <- c(0, 1, 10, 10^6)
rdg_fit <- glmnet(X, y, alpha=0, lambda=lambda_range, intercept=FALSE,
penalty.factor = c(0, rep(1, train_size - 2), 0),
thresh =1e-12, maxit = 10^7)
rdg_fit <- glmnet(X, y, alpha=0, lambda=lambda_range, intercept=FALSE,
thresh =1e-12, maxit = 10^7)
train_size <- length(x)
X <- truncated.power.design.matrix(x)
lambda_range <- c(0, 1, 10, 10^6)
rdg_fit <- glmnet(X, y, alpha=0, lambda=lambda_range, intercept=FALSE,
thresh =1e-12, maxit = 10^7)
yhat1 <- predict(rdg_fit, X, s=0)
yhat2 <- predict(rdg_fit, X, s=1)
library(leaps)
library(scales)
library(glmnet)
source('test-data.r')
truncated.power.design.matrix <- function(x) {
n <- length(x)
results <- matrix(0, n, n)
results[, n] <- 1
knots <- sort(x)
for (i in 1:n) {
x_i = x[i]
for (j in 1:(n-1)) {
knot_j <- knots[j]
if (x_i > knot_j) {
results[i, j] <- x_i - knot_j
} else {
break
}
}
}
return(results)
}
train_size <- length(x)
X <- truncated.power.design.matrix(x)
lambda_range <- c(0, 1, 10, 10^6)
rdg_fit <- glmnet(X, y, alpha=0, lambda=lambda_range, intercept=FALSE,
thresh =1e-12, maxit = 10^7)
yhat1 <- predict(rdg_fit, X, s=0)
yhat2 <- predict(rdg_fit, X, s=1)
yhat3 <- predict(rdg_fit, X, s=10)
plot(x, y, col='gray', pch=16)
plot(x, y, col='gray', pch=16)
lines(x, yhat1, lwd=3, col='skyblue')
lines(x, yhat2, lwd=3, col='orange')
lines(x, yhat3, lwd=3, col='orchid')
yhat4 <- predict(rdg_fit, X, s=10^6)
plot(x, y, col='gray', pch=16)
lines(x, yhat4, lwd=3, col='skyblue')
abline(lm(y~0 + x), col='orange', lty=2, lwd=2)
legend('topright', legend=c('lambda = 10^6', 'linear OLS'),
col=c('skyblue', 'orange'), lty = c(1, 3))
rdg_fit <- glmnet(X, y, alpha=0, lambda=lambda_range, intercept=FALSE,
thresh =1e-12, maxit = 10^7, penalty.factor = c(0,rep(1,train_size-2),0))
yhat1 <- predict(rdg_fit, X, s=0)
yhat2 <- predict(rdg_fit, X, s=1)
yhat3 <- predict(rdg_fit, X, s=10)
plot(x, y, col='gray', pch=16)
lines(x, yhat1, lwd=3, col='skyblue')
plot(x, y, col='gray', pch=16)
lines(x, yhat1, lwd=3, col='skyblue')
lines(x, yhat2, lwd=3, col='orange')
lines(x, yhat3, lwd=3, col='orchid')
plot(x, y, col='gray', pch=16)
lines(x, yhat1, lwd=3, col='skyblue')
lines(x, yhat2, lwd=3, col='orange')
lines(x, yhat3, lwd=3, col='orchid')
yhat4 <- predict(rdg_fit, X, s=10^6)
plot(x, y, col='gray', pch=16)
lines(x, yhat4, lwd=3, col='skyblue')
abline(lm(y~0 + x), col='orange', lty=2, lwd=2)
set.seed(123)
lambda_range <- c(0, 10^{seq(-3, 6, by=0.1)})
cv_fit <- cv.glmnet(X, y, alpha=0, lambda = lambda_range,
thresh =1e-12, maxit = 10^7,
penalty.factor = c(0, rep(1, train_size - 2), 0))
set.seed(123)
cv_fit <- cv.glmnet(X, y, alpha=0, thresh =1e-12, maxit = 10^7,
penalty.factor = c(0, rep(1, train_size - 2), 0))
plot(cv_fit)
lambda_opt <- cv_fit$lambda.min
print(lambda_opt)
rdg_fit_opt <- glmnet(X, y, alpha=0, lambda=lambda_opt, intercept=FALSE,
penalty.factor = c(0, rep(1, train_size - 2), 0))
yhat <- predict(rdg_fit_opt, X)
plot(x, y, col='gray', pch=16)
lines(x, yhat, lwd=3, col='skyblue')
set.seed(123)
cv_fit <- cv.glmnet(X, y, alpha=0, thresh =1e-12, maxit = 10^7,
penalty.factor = c(0, rep(1, train_size - 2), 0))
plot(cv_fit)
lambda_opt <- cv_fit$lambda.min
print(lambda_opt)
rdg_fit_opt <- glmnet(X, y, alpha=0, lambda=lambda_opt, intercept=FALSE,
penalty.factor = c(0, rep(1, train_size - 2), 0))
yhat <- predict(rdg_fit_opt, X)
plot(x, y, col='gray', pch=16)
lines(x, yhat, lwd=3, col='skyblue')
lambda_opt
install.packages(c("gbm", "randomForest", "tree"))
library(tree)
library(ISLR)
attach(Carseats)
High=ifelse(Sales<=8,"No","Yes")
Carseats=data.frame(Carseats,High)
tree.carseats=tree(High~.-Sales,Carseats)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats,pretty=0)
tree.carseats
set.seed(2)
train=sample(1:nrow(Carseats), 200)
Carseats.test=Carseats[-train,]
High.test=High[-train]
tree.carseats=tree(High~.-Sales,Carseats,subset=train)
tree.pred=predict(tree.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
(86+57)/200
library(MASS)
set.seed(1)
train = sample(1:nrow(Boston), nrow(Boston)/2)
tree.boston=tree(medv~.,Boston,subset=train)
summary(tree.boston)
plot(tree.boston)
text(tree.boston,pretty=0)
cv.boston=cv.tree(tree.boston)
plot(cv.boston$size,cv.boston$dev,type='b')
prune.boston=prune.tree(tree.boston,best=5)
plot(prune.boston)
text(prune.boston,pretty=0)
tanh(3)
tanh(1)
a1 = 0.762
a2 = 0.995 + 0.762
exp(a1) / (exp(a1) + exp(a2))
exp(a2) / (exp(a1) + exp(a2))
knitr::opts_chunk$set(echo = TRUE)
p_m1 = seq(from=0, to=1, by=0.01)
classerror_val = 1 - max(p, 1-p)
## 1 - max(p, 1-p)
classerror_val = 1 - max(p_m1, 1-p_m1)
classerror_val
p_m1
max(p_m1, 1-p_m1)
1 - p_m1
max(1, 2)
pmax(p_m1, 1-p_m1)
p_m1 = seq(from=0, to=1, by=0.01)
gini_val = 2 * p_m1 * (1-p_m1)
entropy_val = - (p_m1 * log(p_m1) + (1-p_m1) * log(1-p_m1))
## 1 - max(p, 1-p)
classerror_val = 1 - pmax(p_m1, 1-p_m1)
plot(NA, NA, xlim=c(0, 1), ylim=c(0, 1), xlab='p_m1', ylab='values')
lines(p_m1, gini_val, col='skyblue', lwd=2)
lines(p_m1, entropy_val, col='orange', lwd=2)
lines(p_m1, classerror_val, col='brick' lwd=2)
library("RColorBrewer")
colors = brewer.pal(n = 3, name = "RdBu")
colors
colors = brewer.pal(n = 3, name = "Set1")
plot(NA, NA, xlim=c(0, 1), ylim=c(0, 1), xlab='p_m1', ylab='values')
lines(p_m1, gini_val, col=colors[1], lwd=2)
lines(p_m1, entropy_val, col=colors[2], lwd=2)
lines(p_m1, classerror_val, col=colors[3], lwd=2)
legend('topleft', legend=c('gini', 'entropy', 'classification error'), col=colors, led=2)
plot(NA, NA, xlim=c(0, 1), ylim=c(0, 1), xlab='p_m1', ylab='values')
lines(p_m1, gini_val, col=colors[1], lwd=2)
lines(p_m1, entropy_val, col=colors[2], lwd=2)
lines(p_m1, classerror_val, col=colors[3], lwd=2)
legend('topleft', legend=c('gini', 'entropy', 'classification error'), col=colors, led=2)
p_m1 = seq(from=0, to=1, by=0.01)
gini_val = 2 * p_m1 * (1-p_m1)
entropy_val = - (p_m1 * log(p_m1) + (1-p_m1) * log(1-p_m1))
## 1 - max(p, 1-p)
classerror_val = 1 - pmax(p_m1, 1-p_m1)
colors = brewer.pal(n = 3, name = "Set1")
plot(NA, NA, xlim=c(0, 1), ylim=c(0, 1), xlab='p_m1', ylab='values')
lines(p_m1, gini_val, col=colors[1], lwd=2)
lines(p_m1, entropy_val, col=colors[2], lwd=2)
lines(p_m1, classerror_val, col=colors[3], lwd=2)
legend('topleft', legend=c('gini', 'entropy', 'classification error'), col=colors, led=2)
p_m1 = seq(from=0, to=1, by=0.01)
gini_val = 2 * p_m1 * (1-p_m1)
entropy_val = - (p_m1 * log(p_m1) + (1-p_m1) * log(1-p_m1))
## 1 - max(p, 1-p)
classerror_val = 1 - pmax(p_m1, 1-p_m1)
colors = brewer.pal(n = 3, name = "Set1")
plot(NA, NA, xlim=c(0, 1), ylim=c(0, 1), xlab='p_m1', ylab='values')
lines(p_m1, gini_val, col=colors[1], lwd=2)
lines(p_m1, entropy_val, col=colors[2], lwd=2)
lines(p_m1, classerror_val, col=colors[3], lwd=2)
legend('topleft', legend=c('gini', 'entropy', 'classification error'), col=colors, lwd=2)
getwd()
setwd('/Users/nantang/Google Drive/STAT435/HW/HW4')
knitr::include_graphics("/Users/nantang/Google Drive/STAT435/HW/HW4/8-4-4-a.jpg")
knitr::include_graphics("/Users/nantang/Google Drive/STAT435/HW/HW4/8-4-4-a.jpg")
plot(NA, NA, xlim=x1_lim, xlab='x1', ylim=x2_lim, ylab='x2')
lines(x=x1_lim, y=c(1, 1))
x1_lim = c(-1, 2)
x2_lim = c(0, 3)
plot(NA, NA, xlim=x1_lim, xlab='x1', ylim=x2_lim, ylab='x2')
lines(x=x1_lim, y=c(1, 1))
plot(NA, NA, xlim=x1_lim, xlab='x1', ylim=x2_lim, ylab='x2')
lines(x=x1_lim, y=c(1, 1))
lines(x=c(1, 1), y=c(x2_lim[0, 1]))
plot(NA, NA, xlim=x1_lim, xlab='x1', ylim=x2_lim, ylab='x2')
lines(x=x1_lim, y=c(1, 1))
lines(x=c(1, 1), y=c(x2_lim[0], 1))
x2_lim[0]
x2_lim[1]
plot(NA, NA, xlim=x1_lim, xlab='x1', ylim=x2_lim, ylab='x2')
lines(x=x1_lim, y=c(1, 1))
lines(x=c(1, 1), y=c(x2_lim[1], 1))
plot(NA, NA, xlim=x1_lim, xlab='x1', ylim=x2_lim, ylab='x2')
lines(x=x1_lim, y=c(1, 1))
lines(x=c(1, 1), y=c(x2_lim[1], 1))
lines(x=x1_lim, y=c(2, 2))
lines(x=c(0, 0), y=c(1, 2))
plot(NA, NA, xlim=x1_lim, xlab='x1', ylim=x2_lim, ylab='x2')
axis(2, tick = FALSE)
lines(x=x1_lim, y=c(1, 1))
lines(x=c(1, 1), y=c(x2_lim[1], 1))
lines(x=x1_lim, y=c(2, 2))
lines(x=c(0, 0), y=c(1, 2))
plot(NA, NA, xlim=x1_lim, xlab='x1', ylim=x2_lim, ylab='x2')
axis(2, tick=c(1, 2))
plot(NA, NA, xlim=x1_lim, xlab='x1', ylim=x2_lim, ylab='x2')
axis(2, at=c(1, 2))
lines(x=x1_lim, y=c(1, 1))
lines(x=c(1, 1), y=c(x2_lim[1], 1))
lines(x=x1_lim, y=c(2, 2))
lines(x=c(0, 0), y=c(1, 2))
plot(NA, NA, xlim=x1_lim, xlab='x1', ylim=x2_lim, ylab='x2', xy.coords(NULL))
axis(2, at=c(1, 2))
lines(x=x1_lim, y=c(1, 1))
lines(x=c(1, 1), y=c(x2_lim[1], 1))
lines(x=x1_lim, y=c(2, 2))
lines(x=c(0, 0), y=c(1, 2))
plot(NA, NA, xlim=x1_lim, xlab='x1', ylim=x2_lim, ylab='x2', xy.coords(NULL))
axis(2, at=c(0, 1, 2, 3))
lines(x=x1_lim, y=c(1, 1))
lines(x=c(1, 1), y=c(x2_lim[1], 1))
lines(x=x1_lim, y=c(2, 2))
lines(x=c(0, 0), y=c(1, 2))
plot(NA, NA, xlim=x1_lim, xlab='x1', ylim=x2_lim, ylab='x2')
lines(x=x1_lim, y=c(1, 1))
lines(x=c(1, 1), y=c(x2_lim[1], 1))
lines(x=x1_lim, y=c(2, 2))
lines(x=c(0, 0), y=c(1, 2))
plot(NA, NA, xlim=x1_lim, xlab='x1', ylim=x2_lim, ylab='x2',
xaxt="n", yaxt="n")
axis(side=2, at=seq(0, 3, 1), labels = FALSE)
lines(x=x1_lim, y=c(1, 1))
lines(x=c(1, 1), y=c(x2_lim[1], 1))
lines(x=x1_lim, y=c(2, 2))
lines(x=c(0, 0), y=c(1, 2))
plot(NA, NA, xlim=x1_lim, xlab='x1', ylim=x2_lim, ylab='x2',
xaxt="n", yaxt="n")
axis(side=2, at=c(1,2))
lines(x=x1_lim, y=c(1, 1))
lines(x=c(1, 1), y=c(x2_lim[1], 1))
lines(x=x1_lim, y=c(2, 2))
lines(x=c(0, 0), y=c(1, 2))
plot(NA, NA, xlim=x1_lim, xlab='x1', ylim=x2_lim, ylab='x2',
xaxt="n", yaxt="n")
axis(side=2, at=c(1, 2))
axis(side=1, at=c(0, 1))
lines(x=x1_lim, y=c(1, 1))
lines(x=c(1, 1), y=c(x2_lim[1], 1))
lines(x=x1_lim, y=c(2, 2))
lines(x=c(0, 0), y=c(1, 2))
plot(NA, NA, xlim=x1_lim, xlab='x1', ylim=x2_lim, ylab='x2',
xaxt="n", yaxt="n")
axis(side=2, at=c(1, 2))
axis(side=1, at=c(0, 1))
lines(x=x1_lim, y=c(1, 1))
lines(x=c(1, 1), y=c(x2_lim[1], 1))
lines(x=x1_lim, y=c(2, 2))
lines(x=c(0, 0), y=c(1, 2))
text(x=0, y=0.5, labels = -1.8)
plot(NA, NA, xlim=x1_lim, xlab='x1', ylim=x2_lim, ylab='x2',
xaxt="n", yaxt="n")
axis(side=2, at=c(1, 2))
axis(side=1, at=c(0, 1))
lines(x=x1_lim, y=c(1, 1))
lines(x=c(1, 1), y=c(x2_lim[1], 1))
lines(x=x1_lim, y=c(2, 2))
lines(x=c(0, 0), y=c(1, 2))
text(x=0, y=0.5, labels='-1.80')
text(x=1.25, y=0.5, labels='0.63')
text(x=-0.5, y=1.5, labels='-1.06')
text(x=1, y=1.5, labels='0.21')
text(x=0.5, y=2.5, labels='2.49')
plot(NA, NA, xlim=x1_lim, xlab='x1', ylim=x2_lim, ylab='x2',
xaxt="n", yaxt="n")
axis(side=2, at=c(1, 2))
axis(side=1, at=c(0, 1))
lines(x=x1_lim, y=c(1, 1))
lines(x=c(1, 1), y=c(x2_lim[1], 1))
lines(x=x1_lim, y=c(2, 2))
lines(x=c(0, 0), y=c(1, 2))
text(x=c(0, 1.25, -0.5, 1, 0.5), y=c(0.5, 0.5, 1.5, 1.5, 2.5),
labels=c(-1.8, 0.63, -1.06, 0.21, 2.49))
library("RColorBrewer")
library('tree')
library('randomForest')
source('Carseats-split.R')
set.seed(123)
range_var <- 1:D
library("RColorBrewer")
library('tree')
library('randomForest')
source('Carseats-split.R')
set.seed(123)
## use m = D/3 \approx 3 as number of predictors in each tree
train_rf <- randomForest(formula=Sales~., data=Carseats.train, mtry=3, importance=TRUE)
test_pred <- predict(train_rf, Carseats.test)
test_mse <- mean((test_pred - Carseats.test$Sales)^2)
print(test_mse)
set.seed(123)
range_var <- 1:D
set.seed(123)
D <- ncol(Carseats.train) - 1
train_bag <- randomForest(formula=Sales~., data=Carseats.train, mtry=D, importance=TRUE)
test_pred <- predict(train_bag, Carseats.test)
test_mse <- mean((test_pred - Carseats.test$Sales)^2)
print(test_mse)
importance(train_bag)
varImpPlot(train_bag)
set.seed(123)
range_var <- 1:D
test_mse <- numeric(length(range_var))
for(i in 1:length(range_var)) {
train_rf <- randomForest(formula=Sales~., data=Carseats.train, mtry=range_var[i], importance=TRUE)
test_pred <- predict(train_rf, Carseats.test)
test_mse[i] <- mean((test_pred - Carseats.test$Sales)^2)
}
plot(range_var, test_mse, type='b', pch=16)
best_m <- range_var[which(test_mse == min(test_mse))]
print(best_m)
