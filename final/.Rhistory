data(iris)
X.train <- iris[, 1:4]
y.train <- iris[, 5]
k_ls <- c(1, 3, 7, 11, 15, 21, 27, 35, 43)
zip_train <- read.table('zip-train.dat', sep='')
X.train <- zip_train[,-1]
y.train <- as.factor(zip_train[,1])
length(which(knn.classifier(X.train, y.train, X.train, k.try=5) !=y.train))
49/2000
length(which(knn.classifier(X.train, y.train, X.train, k.try=k_ls) !=y.train))
# define euclidean distance
euc.dist <- function(q_pt, train_pts) {
return(sqrt(rowSums(sweep(as.matrix(train_pts), 2, as.numeric(q_pt))**2)))
#return(apply(train_pts, 1, function(x) sqrt(sum((q_pt - x) ^ 2))))
}
knn.classifier <- function(X.train, y.train, X.test, k.try=1, pi=rep(1/K, K), CV=F) {
test_size <- nrow(X.test)
train_size <- nrow(X.train)
test_pred <- matrix(nrow=test_size, ncol=length(k.try))
K <- length(levels(y.train))
ni <- table(y.train)
for (i in 1:test_size){
query_pt <- X.test[i,]
X.train.final <- X.train
y.train.final <- y.train
# CV only make sense if X.train = X.test
if (CV) {
X.train.final <- X.train.final[-i,]
y.train.final <- y.train.final[-i]
}
eu_dist <- euc.dist(query_pt, X.train.final)
# find k nearest training obs
for (j in 1:length(k.try)) {
knn_index <- which(eu_dist %in% sort(eu_dist)[1:k.try[j]])
knn_train_label <- y.train.final[knn_index]
knn_train_count <- table(knn_train_label)
knn_score <- knn_train_count * pi / ni
max_score <- max(knn_score, na.rm=TRUE)
pred_label <- names(knn_score[which(knn_score == max_score)])
# check if tie exists
if (length(pred_label) > 1) {
# do nothing
print('have a tie')
}
test_pred[i, j] <- pred_label
}
}
return(test_pred)
}
data(iris)
X.train <- iris[, 1:4]
y.train <- iris[, 5]
train_err <- length(which(knn.classifier(X.train, y.train, X.train, k.try=5) !=y.train))
cat('number of misclassification: ', train_err)
train_err_cv <- length(which(knn.classifier(X.train, y.train, X.train, k.try=5, CV=TRUE) !=y.train))
cat('number of misclassification (CV = True): ', train_err_cv)
k_ls <- c(1, 3, 7, 11, 15, 21, 27, 35, 43)
zip_train <- read.table('zip-train.dat', sep='')
X.train <- zip_train[,-1]
y.train <- as.factor(zip_train[,1])
length(which(knn.classifier(X.train, y.train, X.train, k.try=k_ls) !=y.train))
length(which(knn.classifier(X.train, y.train, X.train, k.try=5) !=y.train))
train_pred <- knn.classifier(X.train, y.train, X.train, k.try=c(5, 10))
train_pred
rbind(train_pred, y.train)
cbind(train_pred, y.train)
cbind(train_pred, y.train-1)
cbind(train_pred, as.numeric(y.train))
cbind(train_pred, as.numeric(y.train)-1)
length(which(train_pred[,2] !=y.train))
length(which(train_pred[,1] !=y.train))
length(which(train_pred[,1:2] !=y.train))
which(train_pred[,1:2] !=y.train)
which(train_pred[,1] !=y.train)
k_err <- numeric(length(2))
k_ls <- c(5, 10)
k_err <- numeric(length(k_ls))
for (i in 1: length(k_ls)) {
k_err[i] <- length(which(train_pred[,i] !=y.train))
}
k_err
k_ls <- c(1, 3, 7, 11, 15, 21, 27, 35, 43)
train_pred <- knn.classifier(X.train, y.train, X.train, k.try=k_ls)
train_pred
ncol(train_pred)
# do nothing
pred_label <- pred_label[1]
# define euclidean distance
euc.dist <- function(q_pt, train_pts) {
return(sqrt(rowSums(sweep(as.matrix(train_pts), 2, as.numeric(q_pt))**2)))
#return(apply(train_pts, 1, function(x) sqrt(sum((q_pt - x) ^ 2))))
}
knn.classifier <- function(X.train, y.train, X.test, k.try=1, pi=rep(1/K, K), CV=F) {
test_size <- nrow(X.test)
train_size <- nrow(X.train)
test_pred <- matrix(nrow=test_size, ncol=length(k.try))
K <- length(levels(y.train))
ni <- table(y.train)
for (i in 1:test_size){
query_pt <- X.test[i,]
X.train.final <- X.train
y.train.final <- y.train
# CV only make sense if X.train = X.test
if (CV) {
X.train.final <- X.train.final[-i,]
y.train.final <- y.train.final[-i]
}
eu_dist <- euc.dist(query_pt, X.train.final)
# find k nearest training obs
for (j in 1:length(k.try)) {
knn_index <- which(eu_dist %in% sort(eu_dist)[1:k.try[j]])
knn_train_label <- y.train.final[knn_index]
knn_train_count <- table(knn_train_label)
knn_score <- knn_train_count * pi / ni
max_score <- max(knn_score, na.rm=TRUE)
pred_label <- names(knn_score[which(knn_score == max_score)])
# check if tie exists
if (length(pred_label) > 1) {
# do nothing
pred_label <- pred_label[1]
}
test_pred[i, j] <- pred_label
}
}
return(test_pred)
}
train_pred <- knn.classifier(X.train, y.train, X.train, k.try=k_ls)
k_err <- numeric(length(k_ls))
for (i in 1: length(k_ls)) {
k_err[i] <- length(which(train_pred[,i] !=y.train))
}
k_err
plot(1:length(k_ls), k_err)
train_pred <- knn.classifier(X.train, y.train, X.train, k.try=k_ls, CV=TRUE)
k_err <- numeric(length(k_ls))
for (i in 1: length(k_ls)) {
k_err[i] <- length(which(train_pred[,i] !=y.train))
}
plot(1:length(k_ls), k_err)
plot(1:length(k_ls), k_err)
k_err
X.test <- read.table('zip-test.dat', sep='')
nrow(X.test)
ncol(X.test)
head(X.test)
zip_test <- read.table('zip-test.dat', sep='')
X.test <- zip_test[, -1]
y.test <- as.factor(zip_test[,1])
head(cbind(y.test, y.train))
test_pred <- knn.classifier(X.train, y.train, X.test, k.try=k_ls, CV=TRUE)
test_k_err <- numeric(length(k_ls))
for (i in 1: length(k_ls)) {
test_k_err[i] <- length(which(test_pred[,i] !=y.test))
}
plot(1:length(k_ls), test_k_err)
test_k_err
0.07922 * 4000
?dist
dist(iris[,1:4])
dist(as.matrix(iris[,1:4]))
as.matrix(dist(iris[,1:4]))
dist_matrix <- zeros()
zeros(0)
zeros(1)
X.train <- iris[,1:4]
CV=T
dist_matrix <- numeric(0)
if (CV) {
dsit_matrix <- as.matrix(dist(X.train))
}
dist_matrix <- numeric(0)
if (CV) {
dist_matrix <- as.matrix(dist(X.train))
}
size(dist_matrix)
dim(dist_matrix)
dist_matrix[1,]
dist_matrix[1,1]
dist_matrix[1,2]
dist_matrix[-1,2]
dist_matrix[-1,1]
# define euclidean distance
euc.dist <- function(q_pt, train_pts) {
return(sqrt(rowSums(sweep(as.matrix(train_pts), 2, as.numeric(q_pt))**2)))
}
knn.classifier <- function(X.train, y.train, X.test, k.try=1, pi=rep(1/K, K), CV=F) {
test_size <- nrow(X.test)
train_size <- nrow(X.train)
test_pred <- matrix(nrow=test_size, ncol=length(k.try))
K <- length(levels(y.train))
ni <- table(y.train)
dist_matrix <- numeric(0)
if (CV) {
dist_matrix <- as.matrix(dist(X.train))
}
for (i in 1:test_size){
query_pt <- X.test[i,]
eu_dist <- numeric(0)
# CV only make sense if X.train = X.test
if (CV) {
eu_dist <- dist_matrix[-i, i]
} else {
eu_dist <- euc.dist(query_pt, X.train)
}
# find k nearest training obs
for (j in 1:length(k.try)) {
knn_index <- which(eu_dist %in% sort(eu_dist)[1:k.try[j]])
knn_train_label <- y.train[knn_index]
knn_train_count <- table(knn_train_label)
knn_score <- knn_train_count * pi / ni
max_score <- max(knn_score, na.rm=TRUE)
pred_label <- names(knn_score[which(knn_score == max_score)])
# check if tie exists
if (length(pred_label) > 1) {
# do nothing
pred_label <- pred_label[1]
}
test_pred[i, j] <- pred_label
}
}
return(test_pred)
}
train_err <- length(which(knn.classifier(X.train, y.train, X.train, k.try=5) !=y.train))
data(iris)
X.train <- iris[, 1:4]
y.train <- iris[, 5]
train_err <- length(which(knn.classifier(X.train, y.train, X.train, k.try=5) !=y.train))
cat('number of misclassification: ', train_err)
train_err_cv <- length(which(knn.classifier(X.train, y.train, X.train, k.try=5, CV=TRUE) !=y.train))
cat('number of misclassification (CV = True): ', train_err_cv)
k_ls <- c(1, 3, 7, 11, 15, 21, 27, 35, 43)
zip_train <- read.table('zip-train.dat', sep='')
X.train <- zip_train[,-1]
y.train <- as.factor(zip_train[,1])
train_pred <- knn.classifier(X.train, y.train, X.train, k.try=k_ls, CV=TRUE)
k_err <- numeric(length(k_ls))
for (i in 1: length(k_ls)) {
k_err[i] <- length(which(train_pred[,i] !=y.train))
}
plot(1:length(k_ls), k_err)
train_err <- length(which(knn.classifier(X.train, y.train, X.train, k.try=5) !=y.train))
train_err <- length(which(knn.classifier(X.train, y.train, X.train, k.try=5) !=y.train))
data(iris)
X.train <- iris[, 1:4]
y.train <- iris[, 5]
train_err <- length(which(knn.classifier(X.train, y.train, X.train, k.try=5) !=y.train))
cat('number of misclassification: ', train_err)
train_err_cv <- length(which(knn.classifier(X.train, y.train, X.train, k.try=5, CV=TRUE) !=y.train))
cat('number of misclassification (CV = True): ', train_err_cv)
k_ls <- c(1, 3, 7, 11, 15, 21, 27, 35, 43)
zip_train <- read.table('zip-train.dat', sep='')
X.train <- zip_train[,-1]
y.train <- as.factor(zip_train[,1])
train_pred <- knn.classifier(X.train, y.train, X.train, k.try=k_ls, CV=TRUE)
k_err <- numeric(length(k_ls))
for (i in 1: length(k_ls)) {
k_err[i] <- length(which(train_pred[,i] !=y.train))
}
plot(1:length(k_ls), k_err)
k_err
class(eu_dist)
typeof(eu_dist)
sort(eu_dist)[1:5]
# define euclidean distance
euc.dist <- function(q_pt, train_pts) {
return(sqrt(rowSums(sweep(as.matrix(train_pts), 2, as.numeric(q_pt))**2)))
}
knn.classifier <- function(X.train, y.train, X.test, k.try=1, pi=rep(1/K, K), CV=F) {
test_size <- nrow(X.test)
train_size <- nrow(X.train)
test_pred <- matrix(nrow=test_size, ncol=length(k.try))
K <- length(levels(y.train))
ni <- table(y.train)
dist_matrix <- numeric(0)
if (CV) {
dist_matrix <- as.matrix(dist(X.train))
}
for (i in 1:test_size){
query_pt <- X.test[i,]
eu_dist <- numeric(0)
y.train.final <- y.train
# CV only make sense if X.train = X.test
if (CV) {
eu_dist <- dist_matrix[-i, i]
y.train.final <- y.train.final[-i]
} else {
eu_dist <- euc.dist(query_pt, X.train)
}
# find k nearest training obs
for (j in 1:length(k.try)) {
knn_index <- which(eu_dist %in% sort(eu_dist)[1:k.try[j]])
knn_train_label <- y.train.final[knn_index]
knn_train_count <- table(knn_train_label)
knn_score <- knn_train_count * pi / ni
max_score <- max(knn_score, na.rm=TRUE)
pred_label <- names(knn_score[which(knn_score == max_score)])
# check if tie exists
if (length(pred_label) > 1) {
# do nothing
pred_label <- pred_label[1]
}
test_pred[i, j] <- pred_label
}
}
return(test_pred)
}
data(iris)
X.train <- iris[, 1:4]
y.train <- iris[, 5]
train_err <- length(which(knn.classifier(X.train, y.train, X.train, k.try=5) !=y.train))
cat('number of misclassification: ', train_err)
train_err_cv <- length(which(knn.classifier(X.train, y.train, X.train, k.try=5, CV=TRUE) !=y.train))
cat('number of misclassification (CV = True): ', train_err_cv)
k_ls <- c(1, 3, 7, 11, 15, 21, 27, 35, 43)
zip_train <- read.table('zip-train.dat', sep='')
X.train <- zip_train[,-1]
y.train <- as.factor(zip_train[,1])
train_pred <- knn.classifier(X.train, y.train, X.train, k.try=k_ls, CV=TRUE)
k_err <- numeric(length(k_ls))
for (i in 1: length(k_ls)) {
k_err[i] <- length(which(train_pred[,i] !=y.train))
}
plot(1:length(k_ls), k_err)
?sample
sample(1:2, 1)
sample(1:2, 1)
sample(1:2, 1)
sample(1:2, 1)
pred_label <- c('a', 'b', 'c')
pred_label <- sample(pred_label, 1)
pred_label
pred_label <- c('a', 'b', 'c')
sample(pred_label, 1)
sample(pred_label, 1)
sample(pred_label, 1)
sample(pred_label, 1)
# define euclidean distance
euc.dist <- function(q_pt, train_pts) {
return(sqrt(rowSums(sweep(as.matrix(train_pts), 2, as.numeric(q_pt))**2)))
}
knn.classifier <- function(X.train, y.train, X.test, k.try=1, pi=rep(1/K, K), CV=F) {
test_size <- nrow(X.test)
train_size <- nrow(X.train)
test_pred <- matrix(nrow=test_size, ncol=length(k.try))
K <- length(levels(y.train))
ni <- table(y.train)
dist_matrix <- numeric(0)
if (CV) {
dist_matrix <- as.matrix(dist(X.train))
}
for (i in 1:test_size){
query_pt <- X.test[i,]
eu_dist <- numeric(0)
y.train.final <- y.train
# CV only make sense if X.train = X.test
if (CV) {
eu_dist <- dist_matrix[-i, i]
y.train.final <- y.train.final[-i]
} else {
eu_dist <- euc.dist(query_pt, X.train)
}
# find k nearest training obs
for (j in 1:length(k.try)) {
knn_index <- which(eu_dist %in% sort(eu_dist)[1:k.try[j]])
knn_train_label <- y.train.final[knn_index]
knn_train_count <- table(knn_train_label)
knn_score <- knn_train_count * pi / ni
max_score <- max(knn_score, na.rm=TRUE)
pred_label <- names(knn_score[which(knn_score == max_score)])
# check if tie exists
if (length(pred_label) > 1) {
# random pick
pred_label <- sample(pred_label, 1)
}
test_pred[i, j] <- pred_label
}
}
return(test_pred)
}
data(iris)
X.train <- iris[, 1:4]
y.train <- iris[, 5]
train_err <- length(which(knn.classifier(X.train, y.train, X.train, k.try=5) !=y.train))
cat('number of misclassification: ', train_err)
train_err_cv <- length(which(knn.classifier(X.train, y.train, X.train, k.try=5, CV=TRUE) !=y.train))
cat('number of misclassification (CV = True): ', train_err_cv)
k_ls <- c(1, 3, 7, 11, 15, 21, 27, 35, 43)
zip_train <- read.table('zip-train.dat', sep='')
X.train <- zip_train[,-1]
y.train <- as.factor(zip_train[,1])
train_pred <- knn.classifier(X.train, y.train, X.train, k.try=k_ls, CV=TRUE)
k_err <- numeric(length(k_ls))
for (i in 1: length(k_ls)) {
k_err[i] <- length(which(train_pred[,i] !=y.train))
}
plot(1:length(k_ls), k_err)
set.seed(123)
train_pred <- knn.classifier(X.train, y.train, X.train, k.try=k_ls, CV=TRUE)
k_err <- numeric(length(k_ls))
for (i in 1: length(k_ls)) {
k_err[i] <- length(which(train_pred[,i] !=y.train))
}
plot(1:length(k_ls), k_err)
k_err/4000
plot(1:length(k_ls), k_err)
axis(1, at=1:length(k_ls), labels = k_ls)
plot(1:length(k_ls), k_err, xaxt='n')
plot(1:length(k_ls), k_err, xaxt='n')
axis(1, at=1:length(k_ls), labels = k_ls)
zip_test <- read.table('zip-test.dat', sep='')
X.test <- zip_test[, -1]
y.test <- as.factor(zip_test[,1])
test_pred <- knn.classifier(X.train, y.train, X.test, k.try=k_ls, CV=TRUE)
test_pred <- knn.classifier(X.train, y.train, X.test, k.try=k_ls)
test_k_err <- numeric(length(k_ls))
for (i in 1: length(k_ls)) {
test_k_err[i] <- length(which(test_pred[,i] !=y.test))
}
plot(1:length(k_ls), test_k_err)
k_err <- k_err / nrow(X.test)
plot(1:length(k_ls), k_err, xaxt='n', ylab='Error Rate', xlab='K')
table(X.train)
pi <- table(y.train)
pi
pi <- table(y.train) / length(y.train)
pi
k_ls <- c(1, 3, 7, 11, 15, 21, 27, 35, 43)
zip_train <- read.table('zip-train.dat', sep='')
X.train <- zip_train[,-1]
y.train <- as.factor(zip_train[,1])
pi_new <- table(y.train) / length(y.train)
set.seed(123)
train_pred <- knn.classifier(X.train, y.train, X.train, pi=pi_new, k.try=k_ls, CV=TRUE)
k_err <- numeric(length(k_ls))
for (i in 1: length(k_ls)) {
k_err[i] <- length(which(train_pred[,i] !=y.train))
}
k_err <- k_err / nrow(X.test)
plot(1:length(k_ls), k_err, xaxt='n', ylab='Error Rate', xlab='K')
axis(1, at=1:length(k_ls), labels = k_ls)
k_err
plot(1:length(k_ls), k_err, xaxt='n', ylab='Error Rate', xlab='K')
axis(1, at=1:length(k_ls), labels = k_ls)
which.min(k_err)
cat('Optimal choice of k: ', which.min(k_err), ', with corresponding training error rate: ', min(k_err))
test_pred <- knn.classifier(X.train, y.train, X.test, pi=pi_new, k.try=1)
test_pred
length(which(train_pred[,i] !=y.train))
235/4000
test_pred <- knn.classifier(X.train, y.train, X.test, k.try=1)
length(which(train_pred[,i] !=y.train))
nrow(X.test)
length(which(train_pred[,i] !=y.train)) / nrow(X.test)
0.03487793*2
data(iris)
X.train <- iris[, 1:4]
y.train <- iris[, 5]
set.seed(123)
train_err <- length(which(knn.classifier(X.train, y.train, X.train, k.try=5) !=y.train))
cat('number of misclassification: ', train_err)
train_err_cv <- length(which(knn.classifier(X.train, y.train, X.train, k.try=5, CV=TRUE) !=y.train))
cat('number of misclassification (CV = True): ', train_err_cv)
table(train_err, y.train)
table(knn.classifier(X.train, y.train, X.train, k.try=5, CV=TRUE), y.train)
table(knn.classifier(X.train, y.train, X.train, k.try=5), y.train)
length(which(test_pred[,i] !=y.test)) / nrow(X.test)
test_pred <- knn.classifier(X.train, y.train, X.test, k.try=1)
length(which(test_pred !=y.test)) / nrow(X.test)
test_pred <- knn.classifier(X.train, y.train, X.test, k.try=1)
set.seed(123)
zip_test <- read.table('zip-test.dat', sep='')
X.test <- zip_test[, -1]
y.test <- as.factor(zip_test[,1])
test_pred <- knn.classifier(X.train, y.train, X.test, k.try=1)
data(iris)
X.train <- iris[, 1:4]
y.train <- iris[, 5]
set.seed(123)
train_err <- length(which(knn.classifier(X.train, y.train, X.train, k.try=5) !=y.train))
cat('number of misclassification: ', train_err)
train_err_cv <- length(which(knn.classifier(X.train, y.train, X.train, k.try=5, CV=TRUE) !=y.train))
cat('number of misclassification (CV = True): ', train_err_cv)
k_ls <- c(1, 3, 7, 11, 15, 21, 27, 35, 43)
zip_train <- read.table('zip-train.dat', sep='')
X.train <- zip_train[,-1]
y.train <- as.factor(zip_train[,1])
pi_new <- table(y.train) / length(y.train)
set.seed(123)
train_pred <- knn.classifier(X.train, y.train, X.train, pi=pi_new, k.try=k_ls, CV=TRUE)
k_err <- numeric(length(k_ls))
for (i in 1: length(k_ls)) {
k_err[i] <- length(which(train_pred[,i] !=y.train))
}
k_err <- k_err / nrow(X.test)
cat('Optimal choice of k: ', which.min(k_err), ', with corresponding training error rate: ', min(k_err))
plot(1:length(k_ls), k_err, xaxt='n', ylab='Error Rate', xlab='K')
axis(1, at=1:length(k_ls), labels = k_ls)
set.seed(123)
zip_test <- read.table('zip-test.dat', sep='')
X.test <- zip_test[, -1]
y.test <- as.factor(zip_test[,1])
dim(X.test)
dim(X.train)
test_pred <- knn.classifier(X.train, y.train, X.test, k.try=1)
test_pred
which(test_pred !=y.test))
which(test_pred !=y.test)
length(which(test_pred !=y.test))
159/2007
test_err <- length(which(test_pred !=y.test)) / nrow(X.test)
test_err
cat('test error rate of 1-nearest neighbor classifier: ', test_err)
